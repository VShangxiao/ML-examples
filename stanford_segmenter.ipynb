{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在NLTK中使用斯坦福中文分词器\n",
    "参考[Python自然语言处理实践: 在NLTK中使用斯坦福中文分词器](http://www.52nlp.cn/python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5-%E5%9C%A8nltk%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8)\n",
    "\n",
    "下载的zip打包文件是这个: [Download Stanford Word Segmenter version 2014-08-27](http://nlp.stanford.edu/software/stanford-segmenter-2014-08-27.zip)，下载后解压。\n",
    "\n",
    "为了方便起见，建议首先进入到解压后的斯坦福分词工具目录下：cd stanford-segmenter-2014-08-27，然后在这个目录下启用ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/zhangjun/Documents/py-machinelearning'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhangjun/Documents/py-machinelearning/stanford-segmenter-2014-08-27\n"
     ]
    }
   ],
   "source": [
    "%cd stanford-segmenter-2014-08-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意分词模型，词典等资源在data目录下，使用的是相对路径，在stanford-segmenter-2014-08-27的目录下执行有效。注意，斯坦福中文分词器提供了两个中文分词模型：\n",
    "- ctb.gz是基于宾州中文树库训练的模型\n",
    "- pku.gz是基于北大在2005backoof上提供的人名日报语料库\n",
    "\n",
    "这里选用了pku.gz，方便最后的测试。\n",
    "\n",
    "注意：需要下载slf4j-api-XXX.jar并改名为slf4j-api.jar放在stanford-segmenter-2014-08-27的目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLASSPATH'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segmenter = StanfordSegmenter(path_to_jar=\"stanford-segmenter-3.4.1.jar\",\n",
    "                              path_to_sihan_corpora_dict=\"./data\", \n",
    "                              path_to_model=\"./data/pku.gz\", \n",
    "                              path_to_dict=\"./data/dict-chris6.ser.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\u8fd9 \\u662f \\u65af\\u5766\\u798f \\u4e2d\\u6587 \\u5206\\u8bcd\\u5668 \\u6d4b\\u8bd5\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = u\"这是斯坦福中文分词器测试\"\n",
    "result = segmenter.segment(sentence)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这 是 斯坦福 中文 分词器 测试\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.encode('UTF-8')\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里同时提供了一个segment_file的调用方法，方便直接对文件进行切分，让我们来测试[《中文分词入门之资源》](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90)中介绍的backoff2005的测试集pku_test.utf8，来看看斯坦福分词器的效果：\n",
    "\n",
    "需要下载icwb2-data解压到stanford-segmenter-2014-08-27的目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = segmenter.segment_file('icwb2-data/testing/pku_test.utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outfile = open('pku_outfile', 'w')\n",
    "outfile.write(result.encode('UTF-8'))\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
